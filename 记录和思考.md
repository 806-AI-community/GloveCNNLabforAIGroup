# 项目中的一些记录和思考

## 关于数据归一化

数据归一化目的在于让特征向量间的相差变小，统一数据的量纲。如果数据量纲不统一，那么我们寻求最优解的特征空间，就可以看做是一个椭圆形的，其中大量冈的属性对应的参数有较长的轴。在更新过程中，可能会出现更新过程不是一直朝向极小点更新的，而是呈现‘Z’字型。使用了归一化对齐量纲之后，更新过程就变成了在近似圆形空间，不断向圆心（极值点）迭代的过程：

![image-20240525165957001](C:\Users\Chandery\AppData\Roaming\Typora\typora-user-images\image-20240525165957001.png)

![image-20240525170003553](C:\Users\Chandery\AppData\Roaming\Typora\typora-user-images\image-20240525170003553.png)

在本项目中用了最大最小归一化，是最简单的归一化方式，式子：
$$
x = \frac{x-min}{max-min}
$$
可以把数据都缩放到[0,1]之间。

但是不妨想想，假设两组数据方差相同，最大最小值也相同，均值不同，在最小最大归一化过后就完全相同了，在不同问题中这种情况会不会产生错误？



## 交叉熵损失

### 熵的表达式

$$
Entropy = -\sum{P(x)log_2P(x)}
$$

### 交叉熵表达式

$$
CrossEntropy = -\sum{P(x)log_2Q(x)}
$$

在深度学习中，这里的$P(x)$用GroundTruth，$Q(x)$用预测值。

交叉熵并不是单纯的两个向量的概率分布越相近越好

但是在分类任务中，真实值往往是独热编码的，假设真实值的独热编号是$i$，则这时候的交叉熵表达式就是
$$
CrossEntropy = -log_2Pred(i)
$$
也就是当预测为$i$的概率越大，则交叉熵损失呈指数级别减小。

因此可以看到，交叉熵损失在分类任务中是表现良好的。



### Pytorch框架中的交叉熵函数

使用Pytorch框架时会用到torch.nn中的nn.CrossEntropyLoss函数来计算交叉熵

```python
criterion = nn.CrossEntropyLoss()

Loss = criterion(input, target)
```

**要注意的是交叉熵的两个参数是不具有交换律的，因此input和target千万不能写错位置！！！！**

此外，查看文档和实验可以发现，pytorch框架中的交叉熵对于input是自带一次Softmax操作的：

```python
criterion = nn.CrossEntropyLoss()

input = torch.tensor([0.2,0.3,0.3,0.2])
target = torch.tensor([0.0,1,0,0])

loss2 = criterion(input, target)

input_softmax = nn.Softmax(dim = 0)(input)

exp = torch.exp(input)

input_soft = exp/torch.sum(exp)

loss1 = -math.log(input_softmax[1])

print(loss1 == loss2)

print(input == input_softmax)

print(input_soft == input_softmax)
```

实验结果输出：

![image-20240525175650185](C:\Users\Chandery\AppData\Roaming\Typora\typora-user-images\image-20240525175650185.png)

实验结果表明：

1. pytorch框架中的交叉熵对于input是自带一次Softmax操作
2. 对于一个已经满足Softmax目的条件(非负，和为1)的向量再做Softmax操作，得到的向量不会与原向量相等，但是鉴于交叉熵在分类任务中的作用是使得目标项预测出来的概率越大越好，且Softmax操作不会对向量各维度的大小关系造成变化，**因此在正确性方面，可以不用区分在算Loss之前进行了多少次Softmax操作。**
3. Softmax表达式为

$$
x_i = \frac{e^{x_i}}{\sum{e^{x_k}}}
$$

既然正确性有了保证，我们就应该着眼于训练的收敛性上：

在反向传播的时候，每一个函数的偏导数都会被乘到最终的梯度中，因此每加入一次Softmax就会多乘一个Softmax的偏导数。

因此我们对表达式求导：
$$
\frac{\partial x_i}{\partial(\frac{e^{x_j}}{\sum{e^{x_k}}})}
$$

- $i=j$ 

$$
\frac{\partial x_i}{\partial(\frac{e^{x_i}}{\sum{e^{x_k}}})}=\frac{e^{x_i}\sum{e^{x_k}}-e^{2x_i}}{(\sum{e^{x_k}})^2}=\frac{e^{x_i}}{\sum{e^{x_k}}}\frac{\sum{e^{x_k}}-e^{x_i}}{\sum{e^{x_k}}}=exp(x_i)(1-exp(x_i))
$$



- $i\neq j$

$$
\frac{\partial x_i}{\partial(\frac{e^{x_j}}{\sum{e^{x_k}}})}=\frac{-e^{x_i}e^{x_j}}{(\sum{e^{x_k}})^2}=-\frac{e^{x_i}}{\sum{e^{x_k}}}\frac{e^{x_j}}{\sum{e^{x_k}}}=-exp(x_i)exp(x_j)
$$

因此
$$
\frac{\partial x_i}{\partial(\frac{e^{x_j}}{\sum{e^{x_k}}})} = \left\{
\begin{matrix}
 exp(x_i)(1-exp(x_i))\quad (i=j) \\
 -exp(x_i)exp(x_j)\quad(i\neq j)
\end{matrix}
\right.
$$
最终乘在梯度上的是当前维度对于所有维度的偏导之和，即：
$$
Partial = -exp(x_i)(exp(x_i)-1)\prod_{j\neq i}exp(x_j)
$$
若前面已经做过若干次Softmax操作，$x_i\in[0,1]$，$exp(x_i)\in[1,e]$，因此$|Partial|>1$。

结论：多做一次Softmax操作会让梯度变大

